{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Задание: Реализуйте любой алгоритм семейства Actor-Critic для произвольной среды."
      ],
      "metadata": {
        "id": "VJ-yrpT4vlFE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OKW6jhz6tLH"
      },
      "source": [
        "## Настройка\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byh-Ua5D6tLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f381adf0-0419-4767-a8be-c7058acf99fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Параметры конфигурации для всей установки\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor для прошлых вознаграждений\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v0\")  # Создание среды\n",
        "env.seed(seed)\n",
        "eps = np.finfo(np.float32).eps.item()  # Наименьшее число, такое что 1.0 + eps != 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6DqjH-h6tLI"
      },
      "source": [
        "## Построение модели нейронной сети\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0zVux166tLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb9a7c8-717f-46e3-e25a-5e07a174a40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "inputs = layers.Input(shape=(num_inputs,))\n",
        "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
        "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = layers.Dense(1)(common)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=[action, critic])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGpmueCT6tLJ"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktzg9u2L6tLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabbb7d2-7e71-4208-faf7-c007b55b0d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f5b50090160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f5b50090160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "вознаграждение: 12.72 за эпизод 10\n",
            "вознаграждение: 18.77 за эпизод 20\n",
            "вознаграждение: 31.07 за эпизод 30\n",
            "вознаграждение: 36.69 за эпизод 40\n",
            "вознаграждение: 49.56 за эпизод 50\n",
            "вознаграждение: 60.99 за эпизод 60\n",
            "вознаграждение: 62.52 за эпизод 70\n",
            "вознаграждение: 63.85 за эпизод 80\n",
            "вознаграждение: 95.41 за эпизод 90\n",
            "вознаграждение: 117.55 за эпизод 100\n",
            "вознаграждение: 82.23 за эпизод 110\n",
            "вознаграждение: 57.55 за эпизод 120\n",
            "вознаграждение: 50.51 за эпизод 130\n",
            "вознаграждение: 46.24 за эпизод 140\n",
            "вознаграждение: 42.52 за эпизод 150\n",
            "вознаграждение: 40.36 за эпизод 160\n",
            "вознаграждение: 38.34 за эпизод 170\n",
            "вознаграждение: 41.91 за эпизод 180\n",
            "вознаграждение: 56.79 за эпизод 190\n",
            "вознаграждение: 81.10 за эпизод 200\n",
            "вознаграждение: 94.01 за эпизод 210\n",
            "вознаграждение: 131.98 за эпизод 220\n",
            "Решено в эпизоде 227!\n"
          ]
        }
      ],
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "while True:  # Выполнять до решения\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            # env.render(); Adding this line would show the attempts\n",
        "            # of the agent in a pop up window.\n",
        "\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "\n",
        "            # Предсказываем вероятности действий и предполагаемое\n",
        "            # будущее вознаграждение из состояния среды\n",
        "            action_probs, critic_value = model(state)\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Выборка действий из распределения вероятностей действий\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Примените выбранное действие в нашем окружении\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Обновление вознаграждения за выполнение для проверки условия решения\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Вычислите ожидаемое значение из вознаграждений\n",
        "        # Вычислите ожидаемое значение из вознаграждений\n",
        "        # - На каждом временном шаге каково общее вознаграждение,\n",
        "        #полученное после этого временного шага.\n",
        "        # - Вознаграждения в прошлом дисконтируются путем умножения их на гамму\n",
        "        # - Это метки для нашего критика\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Нормализация\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Вычисление значений потерь для обновления нашей сети\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in history:\n",
        "            # В этот момент истории критик оценил, что в будущем мы получим\n",
        "            # общую награду = `value`. Мы предприняли действие с вероятностью\n",
        "            # log_prob и в итоге получили общую награду = `ret`. Актор должен\n",
        "            # быть обновлен таким образом, чтобы он предсказывал действие,\n",
        "            # которое приведет к высокой награде (по сравнению с оценкой\n",
        "            # критика) с высокой вероятностью.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # Критик должен быть обновлен так, чтобы он предсказывал лучшую\n",
        "            # оценку будущих вознаграждений.\n",
        "            critic_losses.append(\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Обратное распространение\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Очиститка истории потерь и вознаграждений\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"вознаграждение: {:.2f} за эпизод {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "\n",
        "    if running_reward > 150:  # Условие считать задачу решенной\n",
        "        print(\"Решено в эпизоде {}!\".format(episode_count))\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQiQDky36tLM"
      },
      "source": [
        "## Визуализация\n",
        "Ранняя стадия обучения:\n",
        "![Imgur](https://i.imgur.com/5gCs5kH.gif)\n",
        "\n",
        "Поздняя стадия обучения:\n",
        "![Imgur](https://i.imgur.com/5ziiZUD.gif)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}